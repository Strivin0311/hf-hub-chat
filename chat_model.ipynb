{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step1. load the model cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model cards from ./data/model_cards.json\n",
      "There are 200 model repo cards\n"
     ]
    }
   ],
   "source": [
    "from src.io import load_cards\n",
    "data_root = \"./data/\"\n",
    "\n",
    "model_cards = load_cards(data_root, \"model\")\n",
    "print(f\"There are {len(model_cards)} model repo cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= The model card for the repo address: https://huggingface.co/microsoft/phi-2 =========================\n",
      "\n",
      "Model Summary\n",
      "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n",
      "Our model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\n",
      "Intended Uses\n",
      "Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n",
      "QA Format:\n",
      "You can provide the prompt as a standalone question as follows:\n",
      "Write a detailed analogy between mathematics and a lighthouse.\n",
      "where the model generates the text after \".\" . To encourage the model to write more concise answers, you can also try the following QA format using \"Instruct: <prompt>\\nOutput:\"\n",
      "Instruct: Write a detailed analogy between mathematics and a lighthouse.\n",
      "Output: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n",
      "where the model generates the text after \"Output:\".\n",
      "Chat Format:\n",
      "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\n",
      "Bob: Well, have you tried creating a study schedule and sticking to it?\n",
      "Alice: Yes, I have, but it doesn't seem to help much.\n",
      "Bob: Hmm, maybe you should try studying in a quiet environment, like the library.\n",
      "Alice: ...\n",
      "where the model generates the text after the first \"Bob:\".\n",
      "Code Format:\n",
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   primes = []\n",
      "   for num in range(2, n+1):\n",
      "       is_prime = True\n",
      "       for i in range(2, int(math.sqrt(num))+1):\n",
      "           if num % i == 0:\n",
      "               is_prime = False\n",
      "               break\n",
      "       if is_prime:\n",
      "           primes.append(num)\n",
      "   print(primes)\n",
      "where the model generates the text after the comments.\n",
      "Notes:\n",
      "Phi-2 is intended for QA, chat, and code purposes. The model-generated text/code should be treated as a starting point rather than a definitive solution for potential use cases. Users should be cautious when employing these models in their applications.\n",
      "Direct adoption for production tasks without evaluation is out of scope of this project. As a result, the Phi-2 model has not been tested to ensure that it performs adequately for any production-level application. Please refer to the limitation sections of this document for more details.\n",
      "If you are using transformers>=4.36.0, always load the model with trust_remote_code=True to prevent side-effects.\n",
      "Sample Code\n",
      "There are four types of execution mode:\n",
      "FP16 / Flash-Attention / CUDA:\n",
      "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", flash_attn=True, flash_rotary=True, fused_dense=True, device_map=\"cuda\", trust_remote_code=True)\n",
      "FP16 / CUDA:\n",
      "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n",
      "FP32 / CUDA:\n",
      "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cuda\", trust_remote_code=True)\n",
      "FP32 / CPU:\n",
      "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_code=True)\n",
      "To ensure the maximum compatibility, we recommend using the second execution mode (FP16 / CUDA), as follows:\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "torch.set_default_device(\"cuda\")\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
      "\n",
      "inputs = tokenizer('''def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
      "\n",
      "outputs = model.generate(**inputs, max_length=200)\n",
      "text = tokenizer.batch_decode(outputs)[0]\n",
      "print(text)\n",
      "Remark: In the generation function, our model currently does not support beam search (num_beams > 1). Furthermore, in the forward pass of the model, we currently do not support outputting hidden states or attention values, or using custom input embeddings.\n",
      "Limitations of Phi-2\n",
      "Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n",
      "Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n",
      "Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n",
      "Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n",
      "Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n",
      "Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n",
      "Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.\n",
      "Training\n",
      "Model\n",
      "Architecture: a Transformer-based model with next-word prediction objective\n",
      "Context length: 2048 tokens\n",
      "Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n",
      "Training tokens: 1.4T tokens\n",
      "GPUs: 96xA100-80G\n",
      "Training time: 14 days\n",
      "Software\n",
      "PyTorch\n",
      "DeepSpeed\n",
      "Flash-Attention\n",
      "License\n",
      "The model is licensed under the MIT license.\n",
      "Trademarks\n",
      "This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft’s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n",
      "\n",
      "\n",
      "========================= The model card for the repo address: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0 =========================\n",
      "\n",
      "TinyLlama-1.1B\n",
      "https://github.com/jzhang38/TinyLlama\n",
      "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01.\n",
      "We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n",
      "This Model\n",
      "This is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T. We follow HF's Zephyr's training recipe. The model was \" initially fine-tuned on a variant of the UltraChat dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with 🤗 TRL's DPOTrainer on the openbmb/UltraFeedback dataset, which contain 64k prompts and model completions that are ranked by GPT-4.\"\n",
      "How to use\n",
      "You will need the transformers>=4.34 Do check the TinyLlama github page for more information.\n",
      "# Install transformers from source - only needed for versions <= v4.34\n",
      "# pip install git+https://github.com/huggingface/transformers.git\n",
      "# pip install accelerate\n",
      "\n",
      "import torch\n",
      "from transformers import pipeline\n",
      "\n",
      "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
      "\n",
      "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
      "messages = [\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
      "    },\n",
      "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
      "]\n",
      "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
      "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
      "print(outputs[0][\"generated_text\"])\n",
      "# <|system|>\n",
      "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
      "# <|user|>\n",
      "# How many helicopters can a human eat in one sitting?</s>\n",
      "# <|assistant|>\n",
      "# ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.utils import show_cards\n",
    "\n",
    "show_cards(model_cards, type=\"model\", num=2, sample=\"head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2. chat with the model cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= Chatting with the model from its card with the repo: microsoft/phi-2 =========================\n",
      "\n",
      "NO, this model has not been fine-tuned for any specific downstream tasks. It is intended for QA, chat, and code purposes, but it has not undergone fine-tuning for specific tasks like classification, summarization, translation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TinyLlama/TinyLlama-1.1B-Chat-v1.0 =========================\n",
      "\n",
      "NO, this model is fine-tuned for chat generation and not for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mistralai/Mixtral-8x7B-Instruct-v0.1 =========================\n",
      "\n",
      "NO. The Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance, but it does not specify any specific downstream tasks it is fine-tuned for.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: h94/IP-Adapter-FaceID =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is primarily designed for generating images conditioned on face embeddings and text prompts.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: intfloat/e5-mistral-7b-instruct =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is primarily focused on improving text embeddings.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: myshell-ai/OpenVoice =========================\n",
      "\n",
      "NO. OpenVoice is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for voice cloning and speech generation in multiple languages and accents.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TencentARC/LLaMA-Pro-8B =========================\n",
      "\n",
      "YES, LLaMA-Pro is fine-tuned for specific downstream tasks, including programming, mathematics, and general language understanding. It is designed to integrate natural and programming languages, making it suitable for a wide range of NLP tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mistralai/Mixtral-8x7B-v0.1 =========================\n",
      "\n",
      "NO, this model is a pretrained base model and does not have any fine-tuning for specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: dataautogpt3/OpenDalleV1.1 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed for generating visual images based on text prompts.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: ahxt/LiteLlama-460M-1T =========================\n",
      "\n",
      "NO. LiteLlama-460M-1T is not specifically fine-tuned for any downstream tasks. It is a reduced-scale language model trained on a specific dataset and tokenized using GPT2Tokenizer.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: whiterabbitneo/WhiteRabbitNeo-13B =========================\n",
      "\n",
      "NO. The model is not fine-tuned for specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stable-diffusion-xl-base-1.0 =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, such as classification and summarization. This can be inferred from the model card which mentions the model's performance on specific tasks and datasets.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: CultriX/MistralTrix-v1 =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, such as direct preference optimization (DPO) using Intel's dataset for neural-chat-7b-v3-1, which surpasses the original model on several benchmarks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mlabonne/Beyonder-4x7B-v2 =========================\n",
      "\n",
      "YES, the Beyonder-4x7B-v2 model is fine-tuned for specific downstream tasks such as chat, code generation, storywriting, and math problem solving, as indicated by the positive prompts for each expert model in the configuration section.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/sdxl-turbo =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a base model that has not been customized for any particular use case.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mistralai/Mistral-7B-v0.1 =========================\n",
      "\n",
      "NO, Mistral-7B-v0.1 is a pretrained base model and is not fine-tuned for any specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: moreh/MoMo-70B-LoRA-V1.4 =========================\n",
      "\n",
      "YES, the MoMo-70B model is fine-tuned for specific downstream tasks using Supervised Fine-Tuning (SFT) with LoRA, but the specific downstream tasks are not mentioned in the model card.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Pclanglais/Mickey-1928 =========================\n",
      "\n",
      "YES, the model is fine-tuned for generating images of specific characters and scenes from the public domain cartoons of Mickey Mouse from 1928.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: nvidia/parakeet-rnnt-1.1b =========================\n",
      "\n",
      "NO, this model is fine-tuned specifically for automatic speech recognition (ASR) tasks and is not intended for other downstream tasks such as classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mistralai/Mistral-7B-Instruct-v0.2 =========================\n",
      "\n",
      "NO, this model is fine-tuned for instruction-based generation and does not appear to be specifically fine-tuned for any specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: NousResearch/Nous-Hermes-2-SOLAR-10.7B =========================\n",
      "\n",
      "YES, the Nous Hermes 2 - SOLAR 10.7B model is fine-tuned for specific downstream tasks, such as classification, summarization, translation, code generation, and math-problem solving. This is evident from the benchmark results and the example outputs provided.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: meta-llama/Llama-2-7b-chat-hf =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, such as classification, summarization, translation, code generation, and math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stable-video-diffusion-img2vid-xt =========================\n",
      "\n",
      "NO. This model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is a generative image-to-video model trained to generate short video clips from an image conditioning.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: openai/whisper-large-v3 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily trained for automatic speech recognition and speech translation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: hr16/ControlNet-HandRefiner-pruned =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a pruned fp16 version of the ControlNet model in HandRefiner, which is focused on refining malformed hands in generated images through diffusion-based conditional inpainting.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: WizardLM/WizardCoder-33B-V1.1 =========================\n",
      "\n",
      "YES, this model is fine-tuned for code generation and math-problem solving, as indicated by its performance on HumanEval, HumanEval-Plus, MBPP, and MBPP-Plus benchmarks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: runwayml/stable-diffusion-v1-5 =========================\n",
      "\n",
      "NO. This model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for text-to-image generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: lllyasviel/ControlNet-v1-1 =========================\n",
      "\n",
      "NO, the model card does not provide information on whether the model is fine-tuned for specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cloudyu/Mixtral_34Bx2_MoE_60B =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a general language model for generating text based on prompts.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a GGUF format model for text generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: upstage/SOLAR-10.7B-Instruct-v1.0 =========================\n",
      "\n",
      "YES, the model is fine-tuned for single-turn conversation, making it less suitable for multi-turn conversations such as chat.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: HuggingFaceH4/zephyr-7b-beta =========================\n",
      "\n",
      "NO, this model is primarily fine-tuned for chat and conversation generation, and is not specifically fine-tuned for other downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cognitivecomputations/dolphin-2.5-mixtral-8x7b =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, including code generation and math-problem solving. It is trained with a lot of coding data and is very obedient, making it suitable for these tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: jondurbin/bagel-dpo-34b-v0.2 =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, including classification, summarization, translation, code generation, and math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: ronniewy/vicuna_api_parameters =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed for automatically generating interface parameters based on the vicuna model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: lllyasviel/sd_control_collection =========================\n",
      "\n",
      "NO, these models are not fine-tuned for specific downstream tasks. They are community SD control models for users to download flexibly.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TencentARC/LLaMA-Pro-8B-Instruct =========================\n",
      "\n",
      "NO, the model is not specifically fine-tuned for any downstream tasks. It is designed to excel in programming, mathematical reasoning, and general language processing, but it is not tailored for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: meta-llama/Llama-2-7b =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, such as classification, summarization, translation, code generation, and math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: coqui/XTTS-v2 =========================\n",
      "\n",
      "NO. This model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for voice generation and voice cloning across multiple languages.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cognitivecomputations/dolphin-2_6-phi-2 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed for general chat and conversation purposes.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cognitivecomputations/dolphin-2.7-mixtral-8x7b =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, including code generation and math-problem solving. This is evident from the example output provided, which demonstrates the model's ability to solve a LeetCode hard problem in Kotlin.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: eastwind/tinymix-8x1b-chat =========================\n",
      "\n",
      "YES, this model is fine-tuned for downstream tasks such as chat and conversation generation, as indicated by the use of positive prompts and the chat template provided.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: intfloat/multilingual-e5-large =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks such as passage retrieval in open QA, ad-hoc information retrieval, semantic similarity, bitext mining, paraphrase retrieval, linear probing classification, and clustering.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: argilla/notux-8x7b-v1 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a preference-tuned version of a pretrained generative Sparse Mixture of Experts model, but it is not fine-tuned for any specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a pre-trained language model designed for general language understanding and generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/WizardCoder-33B-V1.1-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a language model designed for text generation and completion.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: guoyww/animatediff =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a general-purpose model for generating animated differences between two inputs.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: h94/IP-Adapter =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed to achieve image prompt capability for pre-trained text-to-image diffusion models and can be generalized to other custom models and controllable generation using existing tools.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, such as language model evaluation, chat use-cases, structured output, and role-playing, as mentioned in the model card.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: innofree/PurelyWholesome =========================\n",
      "\n",
      "NO, this model does not appear to be fine-tuned for any specific downstream tasks. It seems to be a general model focused on creating wholesome and naturalistic content.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: meta-llama/Llama-2-7b-hf =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a base model that can be fine-tuned for various tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: teknium/OpenHermes-2.5-Mistral-7B =========================\n",
      "\n",
      "YES, the OpenHermes 2.5 Mistral 7B model is fine-tuned for specific downstream tasks, including code generation and generalist improvements to the model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: google/flan-t5-base =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, including zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, fairness and safety research, and understanding limitations of current large language models.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stable-diffusion-2-1 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily used for text-to-image generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: adept/fuyu-8b =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is intended for research purposes only and may require fine-tuning for specific use cases.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: openchat/openchat-3.5-1210 =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, including coding, general tasks, mathematical reasoning, and evaluator/feedback capabilities.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: upstage/SOLAR-10.7B-v1.0 =========================\n",
      "\n",
      "YES, the model is fine-tuned for specific downstream tasks, as indicated by the usage instructions that mention the need for fine-tuning the model before using it for chatting.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/phi-2-GGUF =========================\n",
      "\n",
      "NO. This model is not fine-tuned for any specific downstream tasks. It is a language model designed for text generation and chat-style conversation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a chat model designed for generating conversational responses.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: gpt2 =========================\n",
      "\n",
      "NO, the model is not fine-tuned for specific downstream tasks. It is pretrained on a large corpus of English data and can be used for text generation or fine-tuned for a downstream task.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Salesforce/blip-image-captioning-large =========================\n",
      "\n",
      "YES, the BLIP model is fine-tuned for specific downstream tasks such as image-text retrieval, image captioning, VQA, and videolanguage tasks. This is mentioned in the abstract of the paper and the model card.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Tyrannosaurus/TinyGPT-V =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a large language model designed for efficient multimodal use with small backbones.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: bert-base-uncased =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily intended to be fine-tuned on a downstream task using the features produced by the BERT model as inputs.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: openai/clip-vit-large-patch14 =========================\n",
      "\n",
      "NO. The CLIP model is not fine-tuned for specific downstream tasks, as it was developed to generalize to arbitrary image classification tasks in a zero-shot manner and was not developed for general model deployment.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: sentence-transformers/all-MiniLM-L6-v2 =========================\n",
      "\n",
      "NO, this model is fine-tuned using a contrastive learning objective and is intended to be used as a sentence and short paragraph encoder for tasks like information retrieval, clustering, or sentence similarity.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: bigcode/starcoder =========================\n",
      "\n",
      "NO, the model license agreement does not indicate that the model is fine-tuned for any specific downstream tasks. It simply outlines the terms of use for the model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: meta-llama/Llama-2-70b-chat-hf =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a base model that can be fine-tuned for various tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: THUDM/chatglm3-6b =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a pre-trained model designed for dialogue generation and general language understanding.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: facebook/seamless-m4t-v2-large =========================\n",
      "\n",
      "YES, the SeamlessM4T model is fine-tuned for specific downstream tasks such as speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition. It is not specifically fine-tuned for tasks like classification, summarization, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stable-zero123 =========================\n",
      "\n",
      "NO. This model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is a latent diffusion model for view-conditioned image generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Norod78/SDXL-YarnArtStyle-LoRA =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It was trained using CivitAI's built-in training feature and is designed for generating images in the Yarn art style.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: tenyx/TenyxChat-7B-v1 =========================\n",
      "\n",
      "NO, this model is fine-tuned for chat and general reasoning, but not for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: CompVis/stable-diffusion-v1-4 =========================\n",
      "\n",
      "NO. The Stable Diffusion v1-4 model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for text-to-image generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF =========================\n",
      "\n",
      "NO. This model is not fine-tuned for any specific downstream tasks. It is a language model that can be used for text generation and completion.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: suno/bark =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a transformer-based text-to-audio model that is primarily designed for generating highly realistic, multilingual speech and other audio.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stable-diffusion-xl-refiner-1.0 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for generating and modifying images based on text prompts.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: berkeley-nest/Starling-LM-7B-alpha =========================\n",
      "\n",
      "YES, the model is fine-tuned for specific downstream tasks such as language model finetuning with reinforcement learning from AI feedback (RLHF) and advantage-induced policy alignment (APA) for language generation and conversation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: playgroundai/playground-v2-1024px-aesthetic =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a diffusion-based text-to-image generative model that generates highly aesthetic images based on text prompts.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a pretrained generative Sparse Mixture of Experts.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: NousResearch/Nous-Hermes-2-Yi-34B =========================\n",
      "\n",
      "YES, Nous Hermes 2 - Yi-34B is fine-tuned for specific downstream tasks, including benchmark results, AGI evaluation, and BigBench reasoning tests.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: jondurbin/nontoxic-bagel-34b-v0.2 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is an experimental fine-tune of yi-34b-200k using bagel and underwent a subset of DPO, but is fairly censored.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: SanjiWatsuki/Kunoichi-7B =========================\n",
      "\n",
      "YES, Kunoichi-7B is fine-tuned for RP (role-playing) tasks, with a focus on maintaining the strengths of Silicon-Maid-7B while increasing intelligence and performance on MT-Bench and EQ-Bench.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Beyonder-4x7B-v2-GGUF =========================\n",
      "\n",
      "NO. This model is not fine-tuned for any specific downstream tasks. It is a language model designed for text generation and conversation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: bigscience/bloom =========================\n",
      "\n",
      "YES, the model can be fine-tuned for specific downstream tasks such as classification, summarization, translation, code generation, and math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Open-Orca/Mistral-7B-OpenOrca =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a language model trained on a curated, filtered subset of data and is designed for general language generation and understanding.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: openchat/openchat_3.5 =========================\n",
      "\n",
      "YES, the OpenChat model is fine-tuned for specific downstream tasks, including chat completion, conversation generation, and language understanding.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/dolphin-2.5-mixtral-8x7b-GGUF =========================\n",
      "\n",
      "NO. This model is not fine-tuned for any specific downstream tasks. It is a language model designed for text generation and conversation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cloudyu/Mixtral_11Bx2_MoE_19B =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a MoE (Mixture of Experts) model designed for generating diverse and high-quality text.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mhenrichsen/danskgpt-tiny =========================\n",
      "\n",
      "NO, DanskGPT-tiny is not fine-tuned for specific downstream tasks. It is a foundation/completion model and is not intended for tasks such as classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: sentence-transformers/all-mpnet-base-v2 =========================\n",
      "\n",
      "NO, this model is fine-tuned using a contrastive learning objective and is intended to be used as a sentence and short paragraph encoder for tasks like information retrieval, clustering, or sentence similarity.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/sd-turbo =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is intended for research purposes and is a generative text-to-image model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Mixtral-8x7B-v0.1-GGUF =========================\n",
      "\n",
      "NO, this model is a pretrained generative Sparse Mixture of Experts and is not fine-tuned for any specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: meta-llama/Llama-2-13b-chat-hf =========================\n",
      "\n",
      "YES, Llama 2 is fine-tuned for specific downstream tasks, such as classification, summarization, translation, code generation, and math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Llama-2-7B-Chat-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a general-purpose language model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: latent-consistency/lcm-lora-sdxl =========================\n",
      "\n",
      "NO, the Latent Consistency Model (LCM) LoRA is not fine-tuned for specific downstream tasks. It is a distilled consistency adapter for stable-diffusion-xl-base-1.0 that allows for reducing the number of inference steps.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: WhereIsAI/UAE-Large-V1 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a universal English sentence embedding model designed for general-purpose use.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: maidalun1020/bce-embedding-base_v1 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily focused on semantic representation, retrieval, and ranking tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/dolphin-2.7-mixtral-8x7b-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a general-purpose language model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a general-purpose language model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: facebook/mbart-large-50-many-to-many-mmt =========================\n",
      "\n",
      "YES, this model is fine-tuned for multilingual machine translation. It is specifically designed to translate between any pair of 50 languages.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: sentinet/suicidality =========================\n",
      "\n",
      "YES, this model is fine-tuned for a specific downstream task, which is text classification. It is trained to distinguish between suicidal and non-suicidal text expressions.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mistralai/Mistral-7B-Instruct-v0.1 =========================\n",
      "\n",
      "YES, this model is fine-tuned for instructing and generating text based on specific instructions provided in the prompt. It is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: HuggingFaceH4/zephyr-7b-alpha =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is primarily a language model fine-tuned on a mix of publicly available, synthetic datasets and is intended for chat purposes.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: microsoft/Orca-2-13b =========================\n",
      "\n",
      "The Orca 2 model is not fine-tuned for specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Nexusflow/NexusRaven-V2-13B =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed for zero-shot function calling and does not have specific fine-tuning for other tasks such as classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: state-spaces/mamba-2.8b-slimpj =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a language model trained on the SlimPajama dataset, but it is not specifically fine-tuned for any particular downstream task.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: DiffusionLight/DiffusionLight =========================\n",
      "\n",
      "YES, this model is fine-tuned for estimating light from any image and generating HDR environment maps using a custom exposure bracketing method.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/WhiteRabbitNeo-13B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a GGUF format model created by WhiteRabbitNeo and quantised using hardware provided by Massed Compute.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: OpenPipe/mistral-ft-optimized-1227 =========================\n",
      "\n",
      "NO, this model is intended to be a strong base suitable for downstream fine-tuning on a variety of tasks, but it is not fine-tuned for any specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mlabonne/phixtral-4x2.8 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a Mixture of Experts (MoE) model made with four different phi-2 models and is designed to perform better than each individual expert.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: facebook/bart-large-cnn =========================\n",
      "\n",
      "YES, this model is fine-tuned for text summarization specifically on the CNN Daily Mail dataset.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed to map sentences and paragraphs to a dense vector space and can be used for tasks like clustering or semantic search.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: WarriorMama777/OrangeMixs =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is primarily designed for generating vivid, flat, anime-style illustrations and does not have specific fine-tuning for other tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: lj1995/VoiceConversionWebUI =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a base model that has not been fine-tuned for any specific task.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: lllyasviel/ControlNet =========================\n",
      "\n",
      "NO. This model is fine-tuned for controlling diffusers using various input modalities such as edge detection, depth estimation, pose detection, scribbles, and semantic segmentation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Bingsu/adetailer =========================\n",
      "\n",
      "NO, this model is fine-tuned for object detection tasks, specifically for detecting faces, hands, and clothing items in images. It is not fine-tuned for other downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cerspense/zeroscope_v2_576w =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is optimized for producing high-quality video compositions and smooth video output.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: segmind/SSD-1B =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for text-to-image generation and does not mention any fine-tuning for other tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: deepseek-ai/deepseek-coder-6.7b-instruct =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, such as code generation and instruction data. It is designed to support project-level code completion and infilling tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: latent-consistency/lcm-lora-sdv1-5 =========================\n",
      "\n",
      "NO, the Latent Consistency Model (LCM) LoRA is not fine-tuned for specific downstream tasks. It is a distilled consistency adapter for stable-diffusion-v1-5 and is designed to reduce the number of inference steps for various tasks, but it is not specifically fine-tuned for any particular downstream task.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stable-video-diffusion-img2vid =========================\n",
      "\n",
      "NO. The model is not fine-tuned for specific downstream tasks, it is intended for research purposes only and is not optimized for any specific applications.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Qwen/Qwen-72B =========================\n",
      "\n",
      "NO. This model is not fine-tuned for specific downstream tasks, it is a large language model that is pretrained on a diverse range of data, but not fine-tuned for specific tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: amused/amused-512 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for text to image generation, image to image generation, and inpainting.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: beowolx/CodeNinja-1.0-OpenChat-7B =========================\n",
      "\n",
      "YES, this model is fine-tuned for code generation, as it has been trained on over 400,000 coding instructions across various languages.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cognitivecomputations/dolphin-2.6-mixtral-8x7b =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks, such as code generation and math-problem solving. It is trained with a lot of coding data and is very good at coding.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cognitivecomputations/dolphin-2.6-mistral-7b =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is trained with a lot of coding data and is not DPO tuned, so it may require encouragement in the system prompt.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/LLaMA-Pro-8B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a general-purpose language model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: impira/layoutlm-document-qa =========================\n",
      "\n",
      "YES, this model is fine-tuned for the task of question answering on documents using both the SQuAD2.0 and DocVQA datasets.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/sd-vae-ft-mse-original =========================\n",
      "\n",
      "NO, this model is fine-tuned for improving the reconstruction of faces and images, but not for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: ggerganov/whisper.cpp =========================\n",
      "\n",
      "NO, the Whisper models are not fine-tuned for specific downstream tasks. They are designed for general-purpose natural language understanding and generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: colbert-ir/colbertv2.0 =========================\n",
      "\n",
      "NO, ColBERT is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily designed for efficient and accurate passage search over large text collections.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: FlagAlpha/Llama2-Chinese-7b-Chat =========================\n",
      "\n",
      "NO, this model is fine-tuned for improving its Chinese conversation ability, not for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: BAAI/bge-large-zh =========================\n",
      "\n",
      "NO, this model is fine-tuned for retrieval, classification, clustering, or semantic search, and can be used in vector databases for LLMs, but not for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Gryphe/MythoMax-L2-13b =========================\n",
      "\n",
      "YES, this model is fine-tuned for both roleplaying and storywriting, making it proficient in those specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Phind/Phind-CodeLlama-34B-v2 =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks related to code generation and programming problem solving. It is proficient in multiple programming languages and has been fine-tuned on a dataset of high-quality programming problems and solutions.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: BAAI/bge-large-en-v1.5 =========================\n",
      "\n",
      "NO. This model is fine-tuned for retrieval, classification, clustering, or semantic search tasks, and can be used in vector databases for LLMs.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/OpenHermes-2.5-Mistral-7B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a language model designed to navigate the complexities of human discourse.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Nous-Capybara-34B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is trained on the Yi-34B model with a 200K context length for 3 epochs on the Capybara dataset, but it is not fine-tuned for specific tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Deci/DeciLM-7B =========================\n",
      "\n",
      "YES, the model is intended for commercial and research use in English and can be fine-tuned for various tasks and languages.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: SciPhi/Sensei-7B-V1 =========================\n",
      "\n",
      "YES, this model is fine-tuned for retrieval-augmented generation (RAG) over detailed web search results, specializing in using search to generate accurate and well-cited summaries from a range of search results.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: jondurbin/bagel-34b-v0.2 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is an experimental fine-tune of yi-34b-200k using bagel, and it is likely better for creative writing, roleplay, etc.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k =========================\n",
      "\n",
      "NO, the model card does not mention any specific downstream tasks for which the model has been fine-tuned. It is described as an open multilingual chatbot without any specific task mentioned.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: rombodawg/Open_Gpt4_8x7B =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a merger of multiple models with the goal of expanding its capabilities and making it more competitive with closed source models like GPT-4.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: NeverSleep/Noromaid-13b-v0.3 =========================\n",
      "\n",
      "YES, this model is fine-tuned for RP, ERP, and general tasks. It has been trained with new datasets and is suitable for various uses.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: jonatasgrosman/wav2vec2-large-xlsr-53-english =========================\n",
      "\n",
      "NO, this model is fine-tuned for speech recognition in English, not for any other specific downstream tasks such as classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: microsoft/table-transformer-detection =========================\n",
      "\n",
      "NO, this model is fine-tuned specifically for table detection, not for any other downstream tasks like classification, summarization, translation, etc.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: hkunlp/instructor-xl =========================\n",
      "\n",
      "YES, the model is fine-tuned for specific downstream tasks, as it can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: THUDM/chatglm2-6b =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a bilingual chat model designed for smooth conversation and low deployment threshold.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: lmsys/vicuna-7b-v1.5 =========================\n",
      "\n",
      "NO, Vicuna is not fine-tuned for specific downstream tasks. It is primarily intended for research on large language models and chatbots.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: jinaai/jina-embeddings-v2-base-en =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a general-purpose text embedding model designed to support a range of use cases, especially when processing long documents is needed.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: XpucT/Deliberate =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed to generate high-quality images based on prompts and trigger words.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: deepseek-ai/deepseek-coder-33b-instruct =========================\n",
      "\n",
      "YES, the model is fine-tuned for specific downstream tasks, such as code generation and instruction data. This is evident from the model summary and the provided example of using the model for chat model inference.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Linaqruf/animagine-xl-2.0 =========================\n",
      "\n",
      "NO. This model is fine-tuned for generating high-resolution anime images from text descriptions, but it is not specifically fine-tuned for any other downstream tasks such as classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stablediffusionapi/juggernaut-xl-v7 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed for generating ultra-realistic images based on the provided prompts.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: ise-uiuc/Magicoder-S-DS-6.7B =========================\n",
      "\n",
      "YES, the Magicoder model is fine-tuned for code generation tasks. It is specifically designed and best suited for coding tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mhdang/dpo-sdxl-text2image-v1 =========================\n",
      "\n",
      "NO, this model is fine-tuned for text-to-image diffusion models alignment using direct preference optimization, not for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: amused/amused-256 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is primarily designed for text to image generation, image to image generation, and inpainting.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: SanjiWatsuki/Silicon-Maid-7B =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a base model that has not been optimized for any particular use case.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: yanolja/KoSOLAR-10.7B-v0.1-deprecated =========================\n",
      "\n",
      "NO, this model has not been fine-tuned for any specific downstream tasks. It was primarily enhanced for Korean language understanding, but no instruction tuning has been performed on this model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mlabonne/Marcoro14-7B-slerp =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a large language model designed for text generation and understanding.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Kquant03/CognitiveFusion-4x7B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a combination of multiple models aimed at improving storytelling and creativity, rather than being optimized for specific tasks like classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Azurro/APT3-1B-Base =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is an autoregressive language model based on the architecture of a transformer and is not intended for deployment without fine-tuning.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: ikawrakow/various-2bit-sota-gguf =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is intended for use with llama.cpp and requires a specific pull request.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: jondurbin/bagel-8x7b-v0.2 =========================\n",
      "\n",
      "The model is fine-tuned for a variety of downstream tasks, including classification, summarization, translation, code generation, and math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mlabonne/NeuralMarcoro14-7B =========================\n",
      "\n",
      "YES, this model is fine-tuned for downstream tasks, specifically for language modeling and chat response generation. This is evident from the training hyperparameters and the usage example provided.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: ORVC/Ov2Super =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed to unleash the full potential of RVC voice-models through fine-tuning, but it is not specifically tailored for any particular task such as classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: ProsusAI/finbert =========================\n",
      "\n",
      "YES, FinBERT is fine-tuned for financial sentiment classification specifically in the finance domain using a large financial corpus and the Financial PhraseBank.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: google/flan-t5-large =========================\n",
      "\n",
      "YES, this model is fine-tuned for specific downstream tasks such as zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, and question answering.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: microsoft/speecht5_tts =========================\n",
      "\n",
      "YES, the model is fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: monster-labs/control_v1p_sd15_qrcode_monster =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is designed to generate creative QR codes that are still scannable, with a focus on scannability and creativity rather than specific downstream tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/control-lora =========================\n",
      "\n",
      "YES, the Control-LoRA model has been fine-tuned for specific downstream tasks such as depth estimation, edge detection, colorization, and image prompt generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: PygmalionAI/mythalion-13b =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is intended for fictional writing and entertainment purposes, and may produce socially unacceptable or factually incorrect text.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: microsoft/phi-1_5 =========================\n",
      "\n",
      "NO, the model is not fine-tuned for specific downstream tasks. It was trained as a language model with a next-word prediction objective and is intended for research purposes.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: BAAI/bge-large-zh-v1.5 =========================\n",
      "\n",
      "NO. This model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is primarily used for tasks like retrieval, classification, clustering, or semantic search, and can be used in vector databases for LLMs.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: BAAI/bge-reranker-large =========================\n",
      "\n",
      "NO. The model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. The model is primarily used for tasks like retrieval, classification, clustering, or semantic search.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stablelm-3b-4e1t =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is intended to be used as a foundational base model for application-specific fine-tuning.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Mistral-7B-OpenOrca-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a language model trained on a curated filtered subset of GPT-4 augmented data.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: liuhaotian/llava-v1.5-13b =========================\n",
      "\n",
      "YES, LLaVA is fine-tuned for specific downstream tasks such as instruction-following and VQA, as mentioned in the model details.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: recogna-nlp/bode-7b-alpaca-pt-br =========================\n",
      "\n",
      "YES. The BODE model is fine-tuned for specific downstream tasks such as text generation, translation, summarization, and more in Portuguese language.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: amazon/MistralLite =========================\n",
      "\n",
      "YES, MistralLite is fine-tuned for specific downstream tasks such as long context line and topic retrieval, summarization, question-answering, and more. This is evident from the model's enhanced capabilities in processing long context and its improved performance on long context handling tasks compared to the original Mistral-7B-Instruct-v0.1 model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: thenlper/gte-large-zh =========================\n",
      "\n",
      "NO, the GTE model is not fine-tuned for specific downstream tasks. It is primarily trained for general text embeddings and can be applied to various downstream tasks of text embeddings, including information retrieval, semantic textual similarity, text reranking, etc.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: openskyml/dalle-3-xl =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a test model similar to DALL•E 3.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stabilityai/stablelm-zephyr-3b =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks like classification, summarization, translation, code generation, or math-problem solving. It is intended to be used as a foundational base model for application-specific fine-tuning.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: 01-ai/Yi-34B-Chat =========================\n",
      "\n",
      "NO, the model is not specifically fine-tuned for any downstream tasks like classification, summarization, translation, code generation, or math-problem solving. The Yi series models are primarily designed as large language models for language understanding and commonsense reasoning.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: mlabonne/NeuralHermes-2.5-Mistral-7B =========================\n",
      "\n",
      "YES, the model is fine-tuned for specific downstream tasks, such as text generation and language modeling, as indicated by the use of Direct Preference Optimization (DPO) and the benchmarks for AGIEval, GPT4All, and TruthfulQA.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: facebook/seamless-streaming =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a multilingual streaming translation model designed for simultaneous translation and streaming automatic speech recognition.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a GPTQ model for text generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: cloudyu/Mixtral_7Bx2_MoE =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a language model designed for causal language modeling.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: knowledgator/comprehend_it-base =========================\n",
      "\n",
      "YES, the model is fine-tuned for specific downstream tasks such as text classification, question-answering, named-entity recognition, relation extraction, and entity linking. It demonstrates better quality on diverse text classification datasets in a zero-shot setting and can be used for multiple information extraction tasks.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/dolphin-2_6-phi-2-GGUF =========================\n",
      "\n",
      "NO. This model is not fine-tuned for any specific downstream tasks. It is a general-purpose language model.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/dolphin-2.6-mistral-7B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a GGUF format model for text generation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: DopeorNope/SOLARC-MOE-10.7Bx6 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a language model based on the SOLAR architecture and is designed for generating text only.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: KnutJaegersberg/Qwen-1_8B-Llamafied =========================\n",
      "\n",
      "NO, the model is not fine-tuned for any specific downstream tasks. The script used to create the model and tokenizer does not indicate any fine-tuning for specific tasks such as classification, summarization, translation, code generation, or math-problem solving.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/phi-2-dpo-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a language model designed for text generation and conversation.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: TheBloke/Open_Gpt4_8x7B-GGUF =========================\n",
      "\n",
      "NO, this model is not fine-tuned for any specific downstream tasks. It is a language model that can be used for various natural language processing tasks, but it has not been fine-tuned for any specific task.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: stanford-oval/Llama-2-7b-WikiChat =========================\n",
      "\n",
      "YES, this model is fine-tuned for the specific downstream task of stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: facebook/bart-large-mnli =========================\n",
      "\n",
      "YES, the bart-large-mnli model is fine-tuned for the specific downstream task of NLI-based zero-shot text classification. It is trained to classify sequences into different class labels based on the entailment and contradiction probabilities.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: microsoft/trocr-base-handwritten =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is primarily intended for optical character recognition (OCR) on single text-line images.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: sentence-transformers/paraphrase-multilingual-mpnet-base-v2 =========================\n",
      "\n",
      "NO, this model is not fine-tuned for specific downstream tasks. It is a sentence-transformers model that maps sentences and paragraphs to a dense vector space and can be used for tasks like clustering or semantic search.\n",
      "\n",
      "========================= Chatting with the model from its card with the repo: Gustavosta/MagicPrompt-Stable-Diffusion =========================\n",
      "\n",
      "YES, this model is fine-tuned for generating prompt texts for imaging AIs, specifically for Stable Diffusion. It was trained with a specific dataset and is intended for this particular task.\n"
     ]
    }
   ],
   "source": [
    "from src.chat import chat_cards\n",
    "\n",
    "each_query = \"Is this model fine-tuned for some specific downstream tasks, \\\n",
    "like classificiation, summarization, translation, code generation, math-problem solving?\\\n",
    "REMEMBER: the response should start with 'YES' or 'NO', and then follow a very brief explanation\"\n",
    "\n",
    "response = chat_cards(\n",
    "    query=each_query,\n",
    "    cards=model_cards,\n",
    "    type=\"model\",\n",
    "    llm=\"gpt-3.5-turbo-1106\",\n",
    "    mode=\"each\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59 / 200 models that are fine-tuned for some specific downstream tasks:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://huggingface.co/TencentARC/LLaMA-Pro-8B',\n",
       " 'https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0',\n",
       " 'https://huggingface.co/CultriX/MistralTrix-v1',\n",
       " 'https://huggingface.co/mlabonne/Beyonder-4x7B-v2',\n",
       " 'https://huggingface.co/moreh/MoMo-70B-LoRA-V1.4',\n",
       " 'https://huggingface.co/Pclanglais/Mickey-1928',\n",
       " 'https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B',\n",
       " 'https://huggingface.co/meta-llama/Llama-2-7b-chat-hf',\n",
       " 'https://huggingface.co/WizardLM/WizardCoder-33B-V1.1',\n",
       " 'https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0',\n",
       " 'https://huggingface.co/cognitivecomputations/dolphin-2.5-mixtral-8x7b',\n",
       " 'https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2',\n",
       " 'https://huggingface.co/meta-llama/Llama-2-7b',\n",
       " 'https://huggingface.co/cognitivecomputations/dolphin-2.7-mixtral-8x7b',\n",
       " 'https://huggingface.co/eastwind/tinymix-8x1b-chat',\n",
       " 'https://huggingface.co/intfloat/multilingual-e5-large',\n",
       " 'https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser',\n",
       " 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B',\n",
       " 'https://huggingface.co/google/flan-t5-base',\n",
       " 'https://huggingface.co/openchat/openchat-3.5-1210',\n",
       " 'https://huggingface.co/upstage/SOLAR-10.7B-v1.0',\n",
       " 'https://huggingface.co/Salesforce/blip-image-captioning-large',\n",
       " 'https://huggingface.co/facebook/seamless-m4t-v2-large',\n",
       " 'https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha',\n",
       " 'https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B',\n",
       " 'https://huggingface.co/SanjiWatsuki/Kunoichi-7B',\n",
       " 'https://huggingface.co/bigscience/bloom',\n",
       " 'https://huggingface.co/openchat/openchat_3.5',\n",
       " 'https://huggingface.co/meta-llama/Llama-2-13b-chat-hf',\n",
       " 'https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt',\n",
       " 'https://huggingface.co/sentinet/suicidality',\n",
       " 'https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1',\n",
       " 'https://huggingface.co/DiffusionLight/DiffusionLight',\n",
       " 'https://huggingface.co/facebook/bart-large-cnn',\n",
       " 'https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct',\n",
       " 'https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B',\n",
       " 'https://huggingface.co/cognitivecomputations/dolphin-2.6-mixtral-8x7b',\n",
       " 'https://huggingface.co/impira/layoutlm-document-qa',\n",
       " 'https://huggingface.co/Gryphe/MythoMax-L2-13b',\n",
       " 'https://huggingface.co/Phind/Phind-CodeLlama-34B-v2',\n",
       " 'https://huggingface.co/Deci/DeciLM-7B',\n",
       " 'https://huggingface.co/SciPhi/Sensei-7B-V1',\n",
       " 'https://huggingface.co/NeverSleep/Noromaid-13b-v0.3',\n",
       " 'https://huggingface.co/hkunlp/instructor-xl',\n",
       " 'https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct',\n",
       " 'https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B',\n",
       " 'https://huggingface.co/mlabonne/NeuralMarcoro14-7B',\n",
       " 'https://huggingface.co/ProsusAI/finbert',\n",
       " 'https://huggingface.co/google/flan-t5-large',\n",
       " 'https://huggingface.co/microsoft/speecht5_tts',\n",
       " 'https://huggingface.co/stabilityai/control-lora',\n",
       " 'https://huggingface.co/liuhaotian/llava-v1.5-13b',\n",
       " 'https://huggingface.co/recogna-nlp/bode-7b-alpaca-pt-br',\n",
       " 'https://huggingface.co/amazon/MistralLite',\n",
       " 'https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B',\n",
       " 'https://huggingface.co/knowledgator/comprehend_it-base',\n",
       " 'https://huggingface.co/stanford-oval/Llama-2-7b-WikiChat',\n",
       " 'https://huggingface.co/facebook/bart-large-mnli',\n",
       " 'https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_models = []\n",
    "\n",
    "for resp in response[0]:\n",
    "    if resp['response'].startswith('YES'):\n",
    "        yes_models.append(resp['repo_addr'])\n",
    "        \n",
    "print(f\"There are {len(yes_models)} / {len(model_cards)} models that are fine-tuned for some specific downstream tasks:\")\n",
    "yes_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io import save_json\n",
    "\n",
    "save_json(yes_models, os.path.join(data_root, \"ft_models.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to answer for these models:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "failed_cards = response[1]\n",
    "print(f\"Failed to answer for these models:\\n{failed_cards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
